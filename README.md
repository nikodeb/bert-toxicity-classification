# Toxicity Classification using BERT
Many online platforms struggle to moderate conversations due to the large commitment required. Effective moderation requires a dedicated team of people whose sole job is to find and remove toxic users and their comments. This leads many online platforms to either limit, or completely shut down user communication.

In this project we present a tool that can help in the fight against online harassment. We fine-tune a pre-trained BERT model that can not only decide whether a comment is toxic, 
but also describe the kind of toxicity that is present. This would allow online platforms to selectively remove the particular kind of toxicity they do not like. 
For example, an online community could be fine with mild swearing, but be completely opposed to obscenities.

This project was originally a Kaggle Notebook.





